# Вычислительная Линейная Алгебра: 


Этот проект содержит реализации методов прикладной линейной алгебры

Каждое задание реализовано в отдельном Python файле (`taskN.py`).
## Оглавление

- [Задания](#задания)
  - [Задание 1: PA=LU разложение и решение СЛАУ (`task1.py`)](#задание-1-palu-разложение-и-решение-слау-task1py)
  - [Задание 2: Нахождение обратной матрицы через PA=LU (`task2.py`)](#задание-2-нахождение-обратной-матрицы-через-palu-task2py)
  - [Задание 3: LDLT разложение и проверка положительной определенности (`task3.py`)](#задание-3-ldlt-разложение-и-проверка-положительной-определенности-task3py)
  - [Задание 4: Разложение Холецкого и решение СЛАУ (`task4.py`)](#задание-4-разложение-холецкого-и-решение-слау-task4py)
  - [Задание 5: LDLT для трехдиагональной матрицы и решение СЛАУ (`task5.py`)](#задание-5-ldlt-для-трехдиагональной-матрицы-и-решение-слау-task5py)
  - [Задание 6: QR разложение (Хаусхолдер) и МНК (`task6.py`)](#задание-6-qr-разложение-хаусхолдер-и-мнк-task6py)
- [Инструкция по запуску](#Инструкция по запуску)
- [Зависимости](#зависимости)
## Задания

---

### Задание 1: PA=LU разложение и решение СЛАУ (`task1.py`)

**Задание:**
> 1. Реализовать разложение Гаусса $PA = LU$ (с выбором ведущего элемента). использовать его для решения линейной системы $Ax = b$. Организовать проверку, вычислив $A\hat{x} - b$, где $\hat{x}$ – найденное решение.

**Теоретическая Часть:**

Используется **метод Гаусса с выбором ведущего элемента по столбцу**.Мы ищем на каждом шаге $k$ максимальный по модулю элемент в $k$-м столбце ниже или на диагонали и меняем текущую строку $k$ с той строкой, где найден этот элемент. Эти перестановки строк можно представить с помощью **матрицы перестановок P**. 
В результате  получаем разложение **PA=LU**, где:
*   $P$ - матрица перестановок (получается из единичной матрицы применением тех же перестановок строк)
*   $L$ - нижняя треугольная матрица с единицами на главной диагонали
*   $U$ - верхняя треугольная матрица

1.  Исходная система: $Ax = b$
2.  Умножаем слева на $P$: $PAx = Pb$
3.  Подставляем $PA=LU$: $LUx = Pb$
4.  Вводим вспомогательный вектор $y = Ux$. Система распадается на две:
    *   $Ly = Pb$ (Решается **прямой подстановкой**)
    *   $Ux = y$ (Решается **обратной подстановкой**)

**Формулы:**

1.  **Разложение:**
    $$ PA = LU $$

2.  **Решение $Ly=c$ (где $c = Pb$), прямая подстановка:**
    $L$ - нижняя треугольная, $L_{ii}=1$.
    $$ y_0 = c_0 $$
    $$ y_i = c_i - \sum_{j=0}^{i-1} L_{ij} y_j \quad (\text{для } i=1, \dots, n-1) $$

3.  **Решение $Ux=y$, обратная подстановка:**
    $U$ - верхняя треугольная.
    $$ x_{n-1} = \frac{y_{n-1}}{U_{n-1, n-1}} $$
    $$ x_i = \frac{1}{U_{ii}} \left( y_i - \sum_{j=i+1}^{n-1} U_{ij} x_j \right) \quad (\text{для } i=n-2, \dots, 0) $$

4.  **Проверка:**
    Вычисляется вектор невязки $r = A\hat{x} - b$

**Примеры:**

*   **Встроенный пример (`e`):**
    *   $A = \begin{bmatrix} 0 & 1 & 1 \\ 2 & 1 & -1 \\ -1 & 1 & -2 \end{bmatrix}, \quad b = \begin{bmatrix} 2 \\ 1 \\ -5 \end{bmatrix}$
    *   Ожидаемое решение: $\hat{x} \approx \begin{bmatrix} 1.375 \\ 0.125 \\ 1.875 \end{bmatrix}$
    *   Ожидаемая невязка: $r\approx \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$
*   **Пример для ручного ввода (`m`):**
    *   $n=2$
    *   Матрица A:
        ```
        Строка 1: 1 1
        Строка 2: 1 -1
        ```
    *   Вектор b: `5 1`
    *   Ожидаемое решение: $\hat{x} = \begin{bmatrix} 3 \\ 2 \end{bmatrix}$
    *   Ожидаемая невязка: $\begin{bmatrix} 0 \\ 0 \end{bmatrix}$


---

### Задание 2: Нахождение обратной матрицы через PA=LU (`task2.py`)

**Задание:**
> 2. Реализовать разложение Гаусса $PA = LU$ (с выбором ведущего элемента), использовать его для нахождения обратной матрицы $A^{-1}$. Организовать проверку, вычислив $A \hat{A}^{-1}$, где $\hat{A}^{-1}$ – найденная обратная матрица.

**Теоретическая Часть:**

Обратная матрица $A^{-1}$ определяется уравнением $A A^{-1} = I$
Обозначим столбцы $A^{-1}$ как $x_1, x_2, \dots, x_n$, а столбцы $I$ как $e_1, e_2, \dots, e_n$ 
Тогда  $A [x_1 | \dots | x_n] = [e_1 | \dots | e_n]$. 
Это эквивалентно $n$ независимым системам линейных уравнений:
$$ Ax_i = e_i, \quad i=1, \dots, n $$
Для нахождения $A^{-1}$  используется разложение $PA=LU$ из Задания 1.
Разложение $PA=LU$ выполняется только один раз.
Для каждого столбца $e_i$:
1.  Вычисляется $c_i = P e_i$.
2.  Решается $L y_i = c_i$ (прямая подстановка).
3.  Решается $U x_i = y_i$ (обратная подстановка).
Вектор $x_i$ является $i$-м столбцом искомой обратной матрицы $A^{-1}$: 
 $A^{-1} = [x_1 | x_2 | \dots | x_n]$
Проверка: Вычисляется $A \hat{A}^{-1}$. В идеальном случае результат $\approx I$.

**Примеры:**

*   **Встроенный пример (`e`):**
    *   $A = \begin{bmatrix} 1 & 2 & 0 \\ 2 & 5 & 1 \\ 0 & 1 & 3 \end{bmatrix}$
    *   Ожидаем обратную матрицу: $\hat{A}^{-1} \approx \begin{bmatrix} 14 & -6 & 2 \\ -6 & 3 & -1 \\ 2 & -1 & 1 \end{bmatrix}$
    *   Проверка: $A \hat{A}^{-1} \approx \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}$
*   **Пример для ручного ввода (`m`):**
    *   $n=2$
    *   Матрица A:
        ```
        Строка 1: 1 1
        Строка 2: 1 -1
        ```
    *   Ожидаем обратную матрицу: $\hat{A}^{-1} = \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & -0.5 \end{bmatrix}$
    *   Проверка: $A \hat{A}^{-1} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$



---

### Задание 3: LDLT разложение и проверка положительной определенности (`task3.py`)

**Задание:**
> 3. Реализовать $LDL^T$-разложение матрицы A, использовать его для проверки положительной определенности матрицы.

**Теоретическая Часть:**

$LDL^T$-разложение применимо к **симметричным** матрицам $A$ ($A=A^T$):
$$ A = L D L^T $$
где:
*   $L$ - **нижняя треугольная матрица с единицами** на главной диагонали ($L_{ii}=1$, $L_{ij}=0$ при $j>i$).
*   $D$ - **диагональная матрица** ($D_{ij}=0$ при $i \neq j$), $d_k = D_{kk}$.


Симметричная матрица $A$ является **положительно определенной**, если $x^T A x > 0$ для любого ненулевого вектора $x$. Симметричная матрица $A$ положительно определена **тогда и только тогда, когда все диагональные элементы $d_k$ в ее $LDL^T$-разложении строго положительны ($d_k > 0$)**.
В алгоритме последовательно вычисляются элементы  $D$ и $L$ и проверяется знак диагональных элементов $d_k$


**Формулы:**

Выводятся из равенства $A_{ij} = (LDL^T)_{ij} = \sum_{k=0}^{\min(i,j)} L_{ik} L_{jk} d_k$:

1.  **Элементы диагональной матрицы D ($d_j = D_{jj}$):**
    Приравнивая $A_{jj} = \sum_{k=0}^{j} L_{jk}^2 d_k = (\sum_{k=0}^{j-1} L_{jk}^2 d_k) + L_{jj}^2 d_j$ и учитывая $L_{jj}=1$:
    $$ d_j = A_{jj} - \sum_{k=0}^{j-1} L_{jk}^2 d_k $$

2.  **Элементы матрицы L ниже диагонали ($L_{ij}$ для $i > j$):**
    Приравнивая $A_{ij} = \sum_{k=0}^{j} L_{ik} L_{jk} d_k = (\sum_{k=0}^{j-1} L_{ik} L_{jk} d_k) + L_{ij} L_{jj} d_j$ и учитывая $L_{jj}=1$:
    $$ L_{ij} = \frac{1}{d_j} \left( A_{ij} - \sum_{k=0}^{j-1} L_{ik} d_k L_{jk} \right) $$
    (нужно $d_j \neq 0$ для вычисления $L_{ij}$)

3.  **Критерий положительной определенности:** $A$ положительно определена $\iff$ все $d_j > 0$ (на практике $d_j > \epsilon > 0$).

**Примеры:**

*   **Встроенные примеры (`t`):**
    *   *Положительно определенная:*
        $A = \begin{bmatrix} 4 & 12 & -16 \\ 12 & 37 & -43 \\ -16 & -43 & 98 \end{bmatrix}$
        Ожидаемый результат: Матрица положительно определена (все $d_i > 0$).
    *   *Не положительно определенная:*
        $A = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 1 & 2 \\ 3 & 2 & 1 \end{bmatrix}$
        Ожидаемый результат: Матрица не положительно определена (найдется $d_i \le 0$).
*   **Пример для ручного ввода (`m`):**
    *   $n=2$
    *   Матрица A:
        ```
        Строка 1: 2 1
        Строка 2: 1 2
        ```
    *   Ожидаемый результат: $d_0=2, L_{10}=1/2=0.5, d_1 = 2 - 0.5^2 \cdot 2 = 1.5$. 
    *  $d_i > 0$  $\implies$ Матрица положительно определена.



---

### Задание 4: Разложение Холецкого и решение СЛАУ (`task4.py`)

**Задание:**
> 4. Реализовать разложение Холецкого для симметричной, положительно определенной матрицы A, использовать его для решения линейной системы $Ax = b$ (если A не является положительно определенной, алгоритм останавливается). Организовать проверку, вычислив $A\hat{x} - b$, где $\hat{x}$ – найденное решение.

**Теоретическая Часть:**

Разложение Холецкого применимо **только к симметричным положительно определенным (СПО)** матрицам $A$ :
$$ A = L L^T $$
где $L$ - **нижняя треугольная матрица** (не обязательно 1 на диагонали)

Для решения системы $Ax=b$ (где $A$ - СПО) с помощью $A=LL^T$:
1.  Подставляем: $LL^T x = b$.
2.  Вводим $y = L^T x$.
3.  Решаем две треугольные системы:
    *   $Ly = b$ (прямая подстановка)
    *   $L^T x = y$ (обратная подстановка)

**Формулы:**

Из равенства $A_{ij} = (LL^T)_{ij} = \sum_{k=0}^{\min(i,j)} L_{ik} L_{jk}$:

1.  **Диагональные элементы $L_{jj}$:**
    Из $A_{jj} = \sum_{k=0}^{j} L_{jk}^2 = (\sum_{k=0}^{j-1} L_{jk}^2) + L_{jj}^2$:
    $$ L_{jj} = \sqrt{A_{jj} - \sum_{k=0}^{j-1} L_{jk}^2} $$
    *Проверка СПО:* Выражение под корнем $A_{jj} - \sum_{k=0}^{j-1} L_{jk}^2$ **должно быть $> 0$**.

2.  **Элементы $L_{ij}$ ниже диагонали ($i > j$):**
    Из $A_{ij} = \sum_{k=0}^{j} L_{ik} L_{jk} = (\sum_{k=0}^{j-1} L_{ik} L_{jk}) + L_{ij} L_{jj}$:
    $$ L_{ij} = \frac{1}{L_{jj}} \left( A_{ij} - \sum_{k=0}^{j-1} L_{ik} L_{jk} \right) $$

3.  **Решение $Ly = b$ (прямая подстановка):**
    $$ y_0 = b_0 / L_{00} $$
    $$ y_i = \frac{1}{L_{ii}} \left( b_i - \sum_{k=0}^{i-1} L_{ik} y_k \right) \quad (\text{для } i=1, \dots, n-1) $$

4.  **Решение $L^T x = y$ (обратная подстановка):**
    $U = L^T$ (верхняя треугольная).
    $$ x_{n-1} = y_{n-1} / U_{n-1, n-1} = y_{n-1} / L_{n-1, n-1} $$
    $$ x_i = \frac{1}{U_{ii}} \left( y_i - \sum_{k=i+1}^{n-1} U_{ik} x_k \right) = \frac{1}{L_{ii}} \left( y_i - \sum_{k=i+1}^{n-1} L_{ki} x_k \right) \quad (\text{для } i=n-2, \dots, 0) $$

**Примеры:**


*   **Встроенные примеры (`t`):**
    *    Симметричная положительно определенная матрица.
        *   $A = \begin{bmatrix} 4. & 12. & -16. \\ 12. & 37. & -43. \\ -16. & -43. & 98. \end{bmatrix}, \quad b = \begin{bmatrix} 1. \\ 2. \\ 3. \end{bmatrix}$
    *   Симметричная, но не положительно определенная матрица
        *   $A = \begin{bmatrix} 1. & 2. & 3. \\ 2. & 1. & 2. \\ 3. & 2. & 1. \end{bmatrix}, \quad b = \begin{bmatrix} 1. \\ 1. \\ 1. \end{bmatrix}$

*   **Пример для ручного ввода (`m`):**
    *   $n=2$
    *   Матрица A:
        ```
        Строка 1: 4 2
        Строка 2: 2 5
        ```
    *   Вектор b: `6 7`
    *   Ожидаемый результат:
         $L = \begin{bmatrix} 2. & 0. \\ 1. & 2. \end{bmatrix}$ , $\hat{x} = \begin{bmatrix} 1. \\ 1. \end{bmatrix}$.
        $A\hat{x} - b = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$



---

### Задание 5: LDLT для трехдиагональной матрицы и решение СЛАУ (`task5.py`)

**Задание:**
> 5. Реализовать $LDL^T$-разложение симметричной трехдиагональной матрицы A. В случае, когда A положительно определена, использовать разложение для решения линейной системы $Ax=b$. Организовать проверку, вычислив $A\hat{x} - b$, где $\hat{x}$ – найденное решение.

**Теоретическая Часть:**

Аналогично Заданию 3, но для **симметричных трехдиагональных матриц**. 
*   Матрица $A$: главная диагональ $a_i$, под- и наддиагональ $c_i$.
*   Матрица $L$: единицы на главной диагонали, под-диагональ $l_i$.
*   Матрица $D$: диагональные элементы $d_i$.

Разложение $A = LDL^T$ и решение $Ax=b$ выполняются в лгоритме за $O(n)$. 
Проверка на положительную определенность : $d_i > 0$ для любого $i$.

**Формулы (для трехдиагонального случая):**

1.  **Разложение $A = LDL^T$:**
    $$ d_0 = a_0 $$
    Для $i=1, \dots, n-1$:
    $$ l_i = c_i / d_{i-1} \quad (= A_{i, i-1} / d_{i-1}) $$
    $$ d_i = a_i - l_i^2 d_{i-1} \quad (= A_{ii} - L_{i, i-1}^2 d_{i-1}) $$
    *Проверка ПО:* $d_i > 0$ для всех $i$.

2.  **Решение $Ly=b$:**
    $$ y_0 = b_0 $$
    $$ y_i = b_i - l_i y_{i-1} \quad (\text{для } i=1, \dots, n-1) $$

3.  **Решение $Dz=y$:**
    $$ z_i = y_i / d_i \quad (\text{для } i=0, \dots, n-1) $$

4.  **Решение $L^T x = z$:**
    $$ x_{n-1} = z_{n-1} $$
    $$ x_i = z_i - l_{i+1} x_{i+1} \quad (\text{для } i=n-2, \dots, 0) $$

**Примеры:**

*   **Встроенный пример (`t`):**
* *Положительно определенная $(n=5)$:*
    *   Главная диагональ $A$: `[2.0, 2.0, 2.0, 2.0, 2.0]`
    *   Под-диагональ $A$: `[-1.0, -1.0, -1.0, -1.0]`
    *   Вектор $b$: `[1.0, 2.0, 3.0, 4.0, 5.0]`
    *  $$A = \begin{bmatrix} 2 & -1 & 0 & 0 & 0 \\ -1 & 2 & -1 & 0 & 0 \\ 0 & -1 & 2 & -1 & 0 \\ 0 & 0 & -1 & 2 & -1 \\ 0 & 0 & 0 & -1 & 2 \end{bmatrix}$$
    *   Ожидаемый результат: Матрица положительно определена, разложение.

* *Не положительно определенная $(n=4)$:* * 
* Главная диагональ $A$ (`a1_i`): `[1.0, 1.0, 1.0, 1.0]`  
* Под-диагональ $A$ (`c1_i`): `[1.0, 1.0, 1.0]` 
 * Вектор $b$ (`b1_i`): `[1.0, 2.0, 3.0, 4.0]`
 $$ A = \begin{bmatrix} 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 1 \end{bmatrix} $$
 
 * Ожидаемый результат:  ($d_3 \le 0$). Матрица не положительно определена. Разложение остановится. Решение системы этим методом невозможно.



*   **Пример для ручного ввода (`m`):**
    *   $n=3$
    *   Главная диагональ A: `2 -1 2`
    *   Под-диагональ A: `-1 1`
    *   Вектор b: `1 0 2`
    *   Ожидаемый результат:
        *   $d_0=2$
        *   $l_1 = -1/2 = -0.5$, $d_1 = -1 - (-0.5)^2 \cdot 2 = -1.5$. **Матрица не положительно определена.** Разложение остановится. Решение системы этим методом невозможно.



---

### Задание 6: QR разложение (Хаусхолдер) и МНК (`task6.py`)

**Текст Задания:**
> 6. Реализовать $QR$-разложение при помощи отражений Хаусхолдера для прямоугольной матрицы полного столбцевого ранга. Решить задачу наименьших квадратов на основе полученного разложения.

**Теоретическая Часть:**

QR-разложение представляет прямоугольную матрицу $A$ ($m \times n$, $m \ge n$) как произведение ортогональной матрицы $Q$ ($m \times m$) и верхней трапециевидной матрицы $R$ ($m \times n$):
$$ A = QR $$
где $Q^T Q = I_m$ и $R = \begin{bmatrix} R_{\text{hat}} \\ 0 \end{bmatrix}$ ($R_{\text{hat}}$ - верхняя треугольная $n \times n$).

Методом Хаусхолдера строится матрица $R$ путем последовательного применения ортогональных матриц отражения $H_k$: $R = H_n \dots H_1 A$. Матрица $Q^T = H_n \dots H_1$.
Полное создание $Q$ не требуется для МНК.

**Отражение Хаусхолдера:** Для обнуления элементов под диагональю в $k$-м столбце текущей матрицы $A'$ используется матрица $H_k = I - \beta_k v_k v_k^T$.
*   В подстолбеце $x = A'_{k:m, k}$.
*   Вычисляем $\alpha = \text{copysign}(||x||_2, x_0)$.
*    $v_k = x + \alpha e_1$ (для подпространства).
*   $\beta_k = 2 / (v_k^T v_k)$.
Применение отражения к матрице $C$(текущая матрица $R$, при $k=0, C=A$) и вектору $c$ :
$$ H_k C = C - \beta_k v_k (v_k^T C) $$
$$ H_k c = c - \beta_k v_k (v_k^T c) $$
$R = H_n \dots H_1 A$, $c = H_n \dots H_1 b$

**Задача наименьших квадратов (МНК):** Минимизировать $||Ax - b||_2$.
Используя QR: $||Ax - b||_2 = ||QRx - b||_2 = ||Rx - Q^T b||_2$.
Обозначим $c = Q^T b$. Минимизируем $||Rx - c||_2$.
$$ ||Rx - c||_2^2 = || \begin{bmatrix} R_{\text{hat}} \\ 0 \end{bmatrix} x - \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} ||_2^2 = ||R_{\text{hat}}x - c_1||_2^2 + ||c_2||_2^2 $$
Минимум достигается при $R_{\text{hat}}x - c_1 = 0$.

 $R_{\text{hat}} x = c_1$

**Алгоритм МНК через QR:**
1.  Применить отражения Хаусхолдера $H_1, \dots, H_n$ к $A$ и $b$ одновременно, чтобы получить $R = H_n \dots H_1 A$ и $c = H_n \dots H_1 b$.
2.  Выделить верхнюю квадратную часть $R_{\text{hat}} = R[0:n, 0:n]$ и первые $n$ компонент $c_1 = c[0:n]$.
3.  Решить верхнетреугольную систему $R_{\text{hat}} x = c_1$ с помощью обратной подстановки.



**Примеры:**

*   **Встроенный пример (`e`):**
    *   $A = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{bmatrix}, \quad b = \begin{bmatrix} 1 \\ 2 \\ 2 \\ 3 \end{bmatrix}$
    *   Ожидаемое МНК решение: $\hat{x} \approx \begin{bmatrix} 0.5 \\ 0.6 \end{bmatrix}$
    *   Ожидаемая норма невязки: $\approx 0.447$
*   **Пример для ручного ввода (`m`):**
    *   $m=3, n=2$
    *   Матрица A:
        ```
        Строка 1: 1 0
        Строка 2: 0 1
        Строка 3: 1 1
        ```
    *   Вектор b: `1 1 3`
    *   Ожидаемое МНК решение: $\hat{x} \approx \begin{bmatrix} 1.333 \\ 1.333 \end{bmatrix}$
    *   Ожидаемая норма невязки: $\approx 0.577$



---

## Инструкция по запуску


1.  **Задание 1 (PA=LU и решение Ax=b):**
    ```bash
    python task1.py
    ```
    Выберите режим: `e` для встроенного примера или `m` для ручного ввода матрицы $A$ и вектора $b$. Выведет разложение P, L, U, решение $x$ и проверку невязки.

2.  **Задание 2 (Обратная матрица через PA=LU):**
    ```bash
    python task2.py
    ```
    Выберите режим: `e` для встроенного примера или `m` для ручного ввода матрицы $A$. Выведет найденную обратную матрицу $\hat{A}^{-1}$ и проверку $A\hat{A}^{-1}$.

3.  **Задание 3 (LDLT и проверка положительной определенности):**
    ```bash
    python task3.py
    ```
    Выберите режим: `t` для запуска тестов на нескольких примерах или `m` для ручного ввода матрицы $A$. Выведет результат проверки на положительную определенность и само $LDL^T$ разложение (если возможно).

4.  **Задание 4 (Разложение Холецкого и решение Ax=b):**
    ```bash
    python task4.py
    ```
    Выберите режим: `t` для запуска тестов на примерах или `m` для ручного ввода матрицы $A$ и вектора $b$. Выведет разложение Холецкого $L$ (если возможно), решение $x$ и проверку невязки.

5.  **Задание 5 (LDLT для трехдиагональной и решение Ax=b):**
    ```bash
    python task5.py
    ```
    Выберите режим: `t` для запуска тестов на примерах или `m` для ручного ввода диагоналей трехдиагональной матрицы $A$ и вектора $b$. Выведет $LDL^T$ разложение (векторы $l_i$ и $d_i$), решение $x$ и проверку невязки.

6.  **Задание 6 (QR Хаусхолдера и МНК):**
    ```bash
    python task6.py
    ```
    Выберите режим: `e` для встроенного примера или `m` для ручного ввода *прямоугольной* матрицы $A$ и вектора $b$. Выведет МНК-решение $x$ и проверку невязки $b - A\hat{x}$.

## Зависимости

*   **Python 3.x**
*   **NumPy:** 
    ```bash
    pip install numpy
    ```